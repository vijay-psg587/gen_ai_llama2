{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBEzV4JhfzMH7OTstMY2jZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijay-psg587/gen_ai_llama2/blob/main/test_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the `PIPELINE` of transfomers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8nXyRi58STD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1O03hPH8Nlu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate huggingface_hub pydantic python_dotenv icecream torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained model - tokenizing, processing and post processing\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZwO353z9Bw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli  logout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv0ichlT-CYA",
        "outputId": "6b35efec-ac23-4f50-8f5d-28d35580dcf0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not logged in!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to Huggingface\n"
      ],
      "metadata": {
        "id": "F2LRQQ-B-qSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the necessary Keys"
      ],
      "metadata": {
        "id": "KCEOz9w7-NCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ[\"HF_API_KEY\"]=\"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\" # this is set a secret in colab\n",
        "os.environ[\"HF_MODEL\"]= \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "## decoder test models\n",
        "os.environ[\"HF_DECODER_BERT_UNCASED\"]=\"bert-base-uncased\"\n",
        "os.environ[\"HF_DECODER_DISTILL_BERT_CHECKPOINT\"] = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "os.environ[\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"] = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "print(os.getenv(\"HF_API_KEY\"))"
      ],
      "metadata": {
        "id": "dKBjnxCl-QY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To access colab secrets\n",
        "from google.colab import userdata\n",
        "print(userdata.get('HF_TOKEN'))\n",
        "\n",
        "import os\n",
        "from icecream import ic\n",
        "try:\n",
        "  import huggingface_hub as hfb\n",
        "  hfb.login(userdata.get('HF_TOKEN'), add_to_git_credential=True)\n",
        "except ValueError as ve:\n",
        "  print(\"Value error:{}\".format(str(ve)))\n",
        "except Exception as e:\n",
        "  print(f\"Common error:{str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNyqn8iP-sZR",
        "outputId": "1aa0ffb4-94df-4849-b863-63288aafd48c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the tokenizer from Transfomer"
      ],
      "metadata": {
        "id": "2EOVys8m-1rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
        "# from transformers import TFAutoModel, PtAutoModel # these are exclusive for the respective model type to be loaded. use AutoModel which determines the model type of its own\n",
        "import os\n",
        "##Fetching the config\n",
        "\n",
        "\n",
        "def load_model(model_name:str|None) -> tuple[any, any]:\n",
        "\n",
        "  if model_name is None or len(model_name)  == 0:\n",
        "    model_name = os.getenv(\"HF_DECODER_BERT_UNCASED\")\n",
        "\n",
        "  config =  AutoConfig.from_pretrained(model_name);\n",
        "  print(config);\n",
        "  print(os.getcwd())\n",
        "  # config.save_pretrained(os.path.join(os.getcwd(),\"LLM/config/BERT_UNCASED\"))\n",
        "  config.save_pretrained(\"./LLM/config/BERT_UNCASED\") # this is to store in colab location\n",
        "\n",
        "  ## Create the tokenizer -  converting the input sequence to high dimenspace vectors\n",
        "  tokenizers = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizers.save_pretrained(\"./LLM/tokenizer/DISTILBERT\")\n",
        "\n",
        "  ## We also create the model from it\n",
        "  model = AutoModel.from_pretrained(model_name)\n",
        "  model.save_pretrained(\"./LLM/model/DISTILBERT\")\n",
        "  return (config, tokenizers)\n",
        "\n",
        "load_model(model_name=os.getenv(\"HF_DECODER_BERT_UNCASED\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_ZBy8Kc-5YV",
        "outputId": "bb71585f-7fcd-4fcf-b896-03cf8013e042"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "/content\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(BertConfig {\n",
              "   \"_name_or_path\": \"bert-base-uncased\",\n",
              "   \"architectures\": [\n",
              "     \"BertForMaskedLM\"\n",
              "   ],\n",
              "   \"attention_probs_dropout_prob\": 0.1,\n",
              "   \"classifier_dropout\": null,\n",
              "   \"gradient_checkpointing\": false,\n",
              "   \"hidden_act\": \"gelu\",\n",
              "   \"hidden_dropout_prob\": 0.1,\n",
              "   \"hidden_size\": 768,\n",
              "   \"initializer_range\": 0.02,\n",
              "   \"intermediate_size\": 3072,\n",
              "   \"layer_norm_eps\": 1e-12,\n",
              "   \"max_position_embeddings\": 512,\n",
              "   \"model_type\": \"bert\",\n",
              "   \"num_attention_heads\": 12,\n",
              "   \"num_hidden_layers\": 12,\n",
              "   \"pad_token_id\": 0,\n",
              "   \"position_embedding_type\": \"absolute\",\n",
              "   \"transformers_version\": \"4.40.2\",\n",
              "   \"type_vocab_size\": 2,\n",
              "   \"use_cache\": true,\n",
              "   \"vocab_size\": 30522\n",
              " },\n",
              " BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              " \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " })"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THis is a sample  for testing\n"
      ],
      "metadata": {
        "id": "Bk0fCJ9X7VA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "import os\n",
        "\n",
        "model = TFAutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_CHECKPOINT\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blf5x1IK7YHV",
        "outputId": "ad24079d-11df-4f34-a9ac-fa368a5dc090"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Sentiment Analysis with Decode Model - DISTILBERT"
      ],
      "metadata": {
        "id": "BeaF9PgT5cEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example with return type as \"tf\" -tensorflow type\n",
        "user_query = [\n",
        "    \"Wow this is awesome. I love to read this book\"\n",
        "    \"The crimes created by the murky and foggy posts really trigger me\"\n",
        "]\n",
        "\n",
        "tf_inputs = tokenizers(user_query, padding=True, truncation=True, return_tensors=\"tf\") # u can have a return value either as \"pt\" or \"tf\"\n",
        "print(tf_inputs)\n",
        "\n",
        "\n",
        "# Example with return type as \"pt\" pytorch\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "pt_inputs = tokenizers(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(pt_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBO1b-8l5hkM",
        "outputId": "15af66da-12d6-41d5-8ed2-c5806b0ee7a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
            "array([[  101, 10166,  2023,  2003, 12476,  1012,  1045,  2293,  2000,\n",
            "         3191,  2023,  2338, 10760,  6997,  2580,  2011,  1996, 14163,\n",
            "        15952,  1998,  9666,  6292,  8466,  2428,  9495,  2033,   102]],\n",
            "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 27), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1]], dtype=int32)>}\n",
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This doesnt work , probably because the return tensors are specifically given in tf and not pt\n",
        "#model = AutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"))\n",
        "\n",
        "# So modified to load from TFAutoModel\n",
        "model = TFAutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"))\n",
        "\n",
        "outputs = model(tf_inputs)\n",
        "#outputs = model(pt_inputs) # this doesnt work because the model loaded is a TF version specifically\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9FWyf5k5j_3",
        "outputId": "0259680b-b665-4baf-ea18-22d668c807ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFBaseModelOutput(last_hidden_state=<tf.Tensor: shape=(1, 27, 768), dtype=float32, numpy=\n",
            "array([[[ 0.5517104 ,  0.12487036,  0.3785873 , ...,  0.23843366,\n",
            "          1.016581  , -0.59104943],\n",
            "        [ 1.0428075 ,  0.6807785 ,  0.17305979, ...,  0.31842172,\n",
            "          1.1339669 , -0.45828193],\n",
            "        [ 0.93357354,  0.40049824,  0.27801323, ...,  0.20877199,\n",
            "          0.99516726, -0.3051931 ],\n",
            "        ...,\n",
            "        [ 0.7711765 ,  0.21543685,  0.24949251, ...,  0.17886403,\n",
            "          0.6729547 , -0.47490284],\n",
            "        [ 0.63072234,  0.19787847,  0.26493877, ...,  0.2734502 ,\n",
            "          0.98380053, -0.43003806],\n",
            "        [ 1.1915655 ,  0.40287334,  0.20719248, ...,  0.3515852 ,\n",
            "          0.47930717, -0.82283175]]], dtype=float32)>, hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading only the specific head -  this reduces the number of logits"
      ],
      "metadata": {
        "id": "U_yemCnbA7hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"))\n",
        "outputs = model(inputs)\n",
        "print(outputs.logits.shape)"
      ],
      "metadata": {
        "id": "5MpwARSzBC3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocessing the output"
      ],
      "metadata": {
        "id": "On9TSPteBMgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "hVYRs2ZeBPXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
        "print(predictions)\n",
        "\n",
        "# to get the labels of each position\n",
        "print(model.config.id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd2nPSlEBXnk",
        "outputId": "8ed01494-c610-4d29-eb26-682f1ba51e98"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[7.3050027e-04 9.9926955e-01]], shape=(1, 2), dtype=float32)\n",
            "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
          ]
        }
      ]
    }
  ]
}