{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPACZJVEQAZuPV4yyLd+wQJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijay-psg587/gen_ai_llama2/blob/feature%2Fmistral/test_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the `PIPELINE` of transfomers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8nXyRi58STD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1O03hPH8Nlu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate huggingface_hub pydantic python_dotenv icecream"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained model - tokenizing, processing and post processing\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZwO353z9Bw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli  logout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv0ichlT-CYA",
        "outputId": "6b35efec-ac23-4f50-8f5d-28d35580dcf0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not logged in!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to Huggingface\n"
      ],
      "metadata": {
        "id": "F2LRQQ-B-qSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the necessary Keys"
      ],
      "metadata": {
        "id": "KCEOz9w7-NCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#os.environ[\"HF_API_KEY\"]=\"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\" # this is set a secret in colab\n",
        "os.environ[\"HF_MODEL\"]= \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "## decoder test models\n",
        "os.environ[\"HF_DECODER_BERT_UNCASED\"]=\"bert-base-uncased\"\n",
        "os.environ[\"HF_DECODER_DISTILL_BERT_CHECKPOINT\"] = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "os.environ[\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"] = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "print(os.getenv(\"HF_API_KEY\"))"
      ],
      "metadata": {
        "id": "dKBjnxCl-QY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To access colab secrets\n",
        "from google.colab import userdata\n",
        "print(userdata.get('HF_TOKEN'))\n",
        "\n",
        "import os\n",
        "from icecream import ic\n",
        "try:\n",
        "  import huggingface_hub as hfb\n",
        "  hfb.login(userdata.get('HF_TOKEN'), add_to_git_credential=True)\n",
        "except ValueError as ve:\n",
        "  print(\"Value error:{}\".format(str(ve)))\n",
        "except Exception as e:\n",
        "  print(f\"Common error:{str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNyqn8iP-sZR",
        "outputId": "1aa0ffb4-94df-4849-b863-63288aafd48c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the tokenizer from Transfomer"
      ],
      "metadata": {
        "id": "2EOVys8m-1rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
        "# from transformers import TFAutoModel, PtAutoModel # these are exclusive for the respective model type to be loaded. use AutoModel which determines the model type of its own\n",
        "import os\n",
        "##Fetching the config\n",
        "config =  AutoConfig.from_pretrained(os.getenv(\"HF_DECODER_BERT_UNCASED\"));\n",
        "print(config);\n",
        "print(os.getcwd())\n",
        "# config.save_pretrained(os.path.join(os.getcwd(),\"LLM/config/BERT_UNCASED\"))\n",
        "config.save_pretrained(\"./LLM/config/BERT_UNCASED\") # this is to store in colab location\n",
        "\n",
        "## Create the tokenizer -  converting the input sequence to high dimenspace vectors\n",
        "tokenizers = AutoTokenizer.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_CHECKPOINT\"))\n",
        "tokenizers.save_pretrained(\"./LLM/tokenizer/DISTILBERT\")\n",
        "\n",
        "## We also create the model from it\n",
        "model = AutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_CHECKPOINT\"))\n",
        "model.save_pretrained(\"./LLM/model/DISTILBERT\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_ZBy8Kc-5YV",
        "outputId": "f6a18768-771c-4e3a-9fe0-a2bf1e62fd6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n",
            "BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.40.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THis is a sample  fpr testing\n"
      ],
      "metadata": {
        "id": "Bk0fCJ9X7VA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "import os\n",
        "\n",
        "model = TFAutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_CHECKPOINT\"))"
      ],
      "metadata": {
        "id": "Blf5x1IK7YHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Sentiment Analysis with Decode Model - DISTILBERT"
      ],
      "metadata": {
        "id": "BeaF9PgT5cEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = [\n",
        "    \"Wow this is awesome. I love to read this book\"\n",
        "    \"The crimes created by the murky and foggy posts really trigger me\"\n",
        "]\n",
        "\n",
        "inputs = tokenizers(user_query, padding=True, truncation=True, return_tensors=\"tf\") # u can have a return value either as \"pt\" or \"tf\"\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "dBO1b-8l5hkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This doesnt work , probably because the return tensors are specifically given in tf and not pt\n",
        "#model = AutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"))\n",
        "\n",
        "# So modified to load from TFAutoModel\n",
        "model = TFAutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"))\n",
        "\n",
        "outputs = model(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "B9FWyf5k5j_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading only the specific head -  this reduces the number of logits"
      ],
      "metadata": {
        "id": "U_yemCnbA7hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"))\n",
        "outputs = model(inputs)\n",
        "print(outputs.logits.shape)"
      ],
      "metadata": {
        "id": "5MpwARSzBC3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocessing the output"
      ],
      "metadata": {
        "id": "On9TSPteBMgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "hVYRs2ZeBPXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
        "print(predictions)\n",
        "\n",
        "# to get the labels of each position\n",
        "print(model.config.id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd2nPSlEBXnk",
        "outputId": "8ed01494-c610-4d29-eb26-682f1ba51e98"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[7.3050027e-04 9.9926955e-01]], shape=(1, 2), dtype=float32)\n",
            "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
          ]
        }
      ]
    }
  ]
}