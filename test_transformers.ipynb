{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmxXgRxnJEr7l2cYkSNDAk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijay-psg587/gen_ai_llama2/blob/main/test_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the `PIPELINE` of transfomers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8nXyRi58STD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1O03hPH8Nlu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate huggingface_hub pydantic python_dotenv icecream"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained model - tokenizing, processing and post processing\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZwO353z9Bw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli  logout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv0ichlT-CYA",
        "outputId": "389fb343-54f1-4e57-9f47-580c1b741a94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not logged in!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to Huggingface\n"
      ],
      "metadata": {
        "id": "F2LRQQ-B-qSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the necessary Keys"
      ],
      "metadata": {
        "id": "KCEOz9w7-NCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_API_KEY\"]=\"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\"\n",
        "os.environ[\"HF_MODEL\"]= \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "## decoder test models\n",
        "os.environ[\"HF_DECODER_BERT_UNCASED\"]=\"bert-base-uncased\"\n",
        "os.environ[\"HF_DECODER_DISTILL_BERT_CHECKPOINT\"] = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "os.environ[\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"] = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "print(os.getenv(\"HF_API_KEY\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKBjnxCl-QY-",
        "outputId": "543aaa60-edcf-479d-e6dc-1b1c6f6c0e7d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from icecream import ic\n",
        "try:\n",
        "  import huggingface_hub as hfb\n",
        "  hfb.login(token=os.getenv(\"HF_API_KEY\"), add_to_git_credential=True)\n",
        "except ValueError as ve:\n",
        "  print(\"Value error:{}\".format(str(ve)))\n",
        "except Exception as e:\n",
        "  print(f\"Common error:{str(e)}\")"
      ],
      "metadata": {
        "id": "CNyqn8iP-sZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the tokenizer from Transfomer"
      ],
      "metadata": {
        "id": "2EOVys8m-1rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
        "# from transformers import TFAutoModel, PtAutoModel # these are exclusive for the respective model type to be loaded. use AutoModel which determines the model type of its own\n",
        "import os\n",
        "##Fetching the config\n",
        "config =  AutoConfig.from_pretrained(os.getenv(\"HF_DECODER_BERT_UNCASED\"));\n",
        "print(config);\n",
        "print(os.getcwd())\n",
        "# config.save_pretrained(os.path.join(os.getcwd(),\"LLM/config/BERT_UNCASED\"))\n",
        "config.save_pretrained(\"./LLM/config/BERT_UNCASED\") # this is to store in colab location\n",
        "\n",
        "## Create the tokenizer -  converting the input sequence to high dimenspace vectors\n",
        "tokenizers = AutoTokenizer.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_CHECKPOINT\"))\n",
        "tokenizers.save_pretrained(\"./LLM/tokenizer/DISTILBERT\")\n",
        "\n",
        "## We also create the model from it\n",
        "model = AutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_CHECKPOINT\"))\n",
        "model.save_pretrained(\"./LLM/model/DISTILBERT\")\n"
      ],
      "metadata": {
        "id": "l_ZBy8Kc-5YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THis is a sample  fpr testing\n"
      ],
      "metadata": {
        "id": "Bk0fCJ9X7VA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "import os\n",
        "\n",
        "model = TFAutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_CHECKPOINT\"))"
      ],
      "metadata": {
        "id": "Blf5x1IK7YHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Sentiment Analysis with Decode Model - DISTILBERT"
      ],
      "metadata": {
        "id": "BeaF9PgT5cEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = [\n",
        "    \"Wow this is awesome. I love to read this book\"\n",
        "    \"The crimes created by the murky and foggy posts really trigger me\"\n",
        "]\n",
        "\n",
        "inputs = tokenizers(user_query, padding=True, truncation=True, return_tensors=\"tf\") # u can have a return value either as \"pt\" or \"tf\"\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "dBO1b-8l5hkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This doesnt work , probably because the return tensors are specifically given in tf and not pt\n",
        "#model = AutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"))\n",
        "\n",
        "# So modified to load from TFAutoModel\n",
        "model = TFAutoModel.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"))\n",
        "\n",
        "outputs = model(inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "B9FWyf5k5j_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading only the specific head -  this reduces the number of logits"
      ],
      "metadata": {
        "id": "U_yemCnbA7hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(os.getenv(\"HF_DECODER_DISTILL_BERT_FINE_TUNED\"))\n",
        "outputs = model(inputs)\n",
        "print(outputs.logits.shape)"
      ],
      "metadata": {
        "id": "5MpwARSzBC3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocessing the output"
      ],
      "metadata": {
        "id": "On9TSPteBMgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "hVYRs2ZeBPXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
        "print(predictions)\n",
        "\n",
        "# to get the labels of each position\n",
        "print(model.config.id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd2nPSlEBXnk",
        "outputId": "8ed01494-c610-4d29-eb26-682f1ba51e98"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[7.3050027e-04 9.9926955e-01]], shape=(1, 2), dtype=float32)\n",
            "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
          ]
        }
      ]
    }
  ]
}